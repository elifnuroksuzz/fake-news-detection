"""
data_preprocessing.py
Profesyonel veri indirme, temizleme ve Ã¶n iÅŸleme modÃ¼lÃ¼
LIAR Dataset iÃ§in optimize edilmiÅŸ
"""

import os
import re
import pandas as pd
import numpy as np
import requests
from typing import Tuple, List, Dict, Optional
import yaml
from tqdm import tqdm
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import warnings
warnings.filterwarnings('ignore')

class DataPreprocessor:
    """
    Profesyonel veri Ã¶n iÅŸleme sÄ±nÄ±fÄ±
    LIAR Dataset iÃ§in Ã¶zelleÅŸtirilmiÅŸ
    """
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        Veri Ã¶n iÅŸleyiciyi baÅŸlat
        
        Args:
            config_path (str): KonfigÃ¼rasyon dosyasÄ± yolu
        """
        self.config = self._load_config(config_path)
        self.stemmer = PorterStemmer()
        self._setup_nltk()
        
        # LIAR Dataset sÃ¼tun isimleri
        self.liar_columns = [
            'label', 'statement', 'subject', 'speaker', 'job_title',
            'state_info', 'party_affiliation', 'barely_true_counts',
            'false_counts', 'half_true_counts', 'mostly_true_counts',
            'pants_on_fire_counts', 'context'
        ]
        
        print("ğŸ”§ DataPreprocessor baÅŸlatÄ±ldÄ±!")
        
    def _load_config(self, config_path: str) -> Dict:
        """KonfigÃ¼rasyon dosyasÄ±nÄ± yÃ¼kle"""
        try:
            with open(config_path, 'r', encoding='utf-8') as file:
                return yaml.safe_load(file)
        except FileNotFoundError:
            print(f"âš ï¸ KonfigÃ¼rasyon dosyasÄ± bulunamadÄ±: {config_path}")
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """VarsayÄ±lan konfigÃ¼rasyon"""
        return {
            'data': {
                'raw_data_urls': {
                    'train': "https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/train.tsv",
                    'valid': "https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/valid.tsv",
                    'test': "https://raw.githubusercontent.com/thiagorainmaker77/liar_dataset/master/test.tsv"
                },
                'raw_data_path': "data/raw/",
                'processed_data_path': "data/processed/",
                'text_cleaning': {
                    'remove_urls': True,
                    'remove_mentions': True,
                    'remove_hashtags': False,
                    'remove_punctuation': True,
                    'convert_lowercase': True,
                    'remove_stopwords': True,
                    'min_word_length': 2
                }
            }
        }
    
    def _setup_nltk(self):
        """NLTK gerekli paketlerini indir"""
        try:
            nltk.data.find('tokenizers/punkt')
            nltk.data.find('corpora/stopwords')
        except LookupError:
            print("ğŸ“¥ NLTK paketleri indiriliyor...")
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            print("âœ… NLTK paketleri hazÄ±r!")
    
    def download_liar_dataset(self) -> bool:
        """
        LIAR Dataset'i indir
        
        Returns:
            bool: Ä°ndirme baÅŸarÄ±lÄ± mÄ±
        """
        print("ğŸ“¥ LIAR Dataset indiriliyor...")
        
        # Raw data klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        raw_path = self.config['data']['raw_data_path']
        os.makedirs(raw_path, exist_ok=True)
        
        urls = self.config['data']['raw_data_urls']
        success_count = 0
        
        for dataset_name, url in urls.items():
            try:
                print(f"ğŸ“¥ {dataset_name}.tsv indiriliyor...")
                
                response = requests.get(url, timeout=30)
                response.raise_for_status()
                
                file_path = os.path.join(raw_path, f"{dataset_name}.tsv")
                with open(file_path, 'wb') as file:
                    file.write(response.content)
                
                print(f"âœ… {dataset_name}.tsv indirildi: {file_path}")
                success_count += 1
                
            except Exception as e:
                print(f"âŒ {dataset_name}.tsv indirilemedi: {str(e)}")
        
        if success_count == len(urls):
            print("ğŸ‰ TÃ¼m veri dosyalarÄ± baÅŸarÄ±yla indirildi!")
            return True
        else:
            print(f"âš ï¸ {success_count}/{len(urls)} dosya indirildi")
            return False
    
    def load_liar_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        LIAR Dataset'i yÃ¼kle
        
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: train, valid, test dataframes
        """
        print("ğŸ“Š LIAR Dataset yÃ¼kleniyor...")
        
        raw_path = self.config['data']['raw_data_path']
        
        try:
            # TSV dosyalarÄ±nÄ± yÃ¼kle
            train_df = pd.read_csv(
                os.path.join(raw_path, "train.tsv"), 
                sep='\t', 
                names=self.liar_columns,
                encoding='utf-8'
            )
            
            valid_df = pd.read_csv(
                os.path.join(raw_path, "valid.tsv"), 
                sep='\t', 
                names=self.liar_columns,
                encoding='utf-8'
            )
            
            test_df = pd.read_csv(
                os.path.join(raw_path, "test.tsv"), 
                sep='\t', 
                names=self.liar_columns,
                encoding='utf-8'
            )
            
            print(f"âœ… Veriler yÃ¼klendi:")
            print(f"   ğŸ“ˆ Train: {len(train_df):,} Ã¶rnĞµĞº")
            print(f"   ğŸ“Š Valid: {len(valid_df):,} Ã¶rnĞµĞº")
            print(f"   ğŸ§ª Test: {len(test_df):,} Ã¶rnÉ™k")
            
            return train_df, valid_df, test_df
            
        except Exception as e:
            print(f"âŒ Veri yÃ¼klenirken hata: {str(e)}")
            return None, None, None
    
    def clean_text(self, text: str) -> str:
        """
        Metin temizleme fonksiyonu
        
        Args:
            text (str): Temizlenecek metin
            
        Returns:
            str: TemizlenmiÅŸ metin
        """
        if pd.isna(text) or not isinstance(text, str):
            return ""
        
        config = self.config['data']['text_cleaning']
        
        # URL'leri kaldÄ±r
        if config.get('remove_urls', True):
            text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Mention'larÄ± kaldÄ±r (@username)
        if config.get('remove_mentions', True):
            text = re.sub(r'@\w+', '', text)
        
        # Hashtag'leri kaldÄ±r (#hashtag)
        if config.get('remove_hashtags', False):
            text = re.sub(r'#\w+', '', text)
        
        # KÃ¼Ã§Ã¼k harfe Ã§evir
        if config.get('convert_lowercase', True):
            text = text.lower()
        
        # Noktalama iÅŸaretlerini kaldÄ±r
        if config.get('remove_punctuation', True):
            text = re.sub(r'[^\w\s]', ' ', text)
        
        # Fazla boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize_and_filter(self, text: str) -> List[str]:
        """
        Metni tokenize et ve filtrele
        
        Args:
            text (str): Ä°ÅŸlenecek metin
            
        Returns:
            List[str]: FiltrelenmiÅŸ token listesi
        """
        if not text:
            return []
        
        config = self.config['data']['text_cleaning']
        
        # Tokenize et
        tokens = word_tokenize(text)
        
        # Stopwords'leri kaldÄ±r
        if config.get('remove_stopwords', True):
            stop_words = set(stopwords.words('english'))
            tokens = [token for token in tokens if token.lower() not in stop_words]
        
        # Minimum kelime uzunluÄŸu filtresi
        min_length = config.get('min_word_length', 2)
        tokens = [token for token in tokens if len(token) >= min_length]
        
        # Sadece alfabetik karakterler
        tokens = [token for token in tokens if token.isalpha()]
        
        return tokens
    
    def preprocess_dataframe(self, df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
        """
        DataFrame'i Ã¶n iÅŸleme
        
        Args:
            df (pd.DataFrame): Ä°ÅŸlenecek dataframe
            dataset_name (str): Dataset adÄ± (train/valid/test)
            
        Returns:
            pd.DataFrame: Ä°ÅŸlenmiÅŸ dataframe
        """
        print(f"ğŸ”„ {dataset_name} verisi Ã¶n iÅŸleme alÄ±nÄ±yor...")
        
        df_processed = df.copy()
        
        # Eksik deÄŸerleri doldur
        df_processed['statement'] = df_processed['statement'].fillna('')
        df_processed['subject'] = df_processed['subject'].fillna('unknown')
        df_processed['speaker'] = df_processed['speaker'].fillna('unknown')
        
        # Ana metin temizleme
        print("   ğŸ§¹ Metinler temizleniyor...")
        df_processed['cleaned_statement'] = df_processed['statement'].progress_apply(self.clean_text)
        
        # Tokenize etme
        print("   ğŸ”¤ Tokenize ediliyor...")
        df_processed['tokens'] = df_processed['cleaned_statement'].progress_apply(self.tokenize_and_filter)
        
        # Token sayÄ±sÄ±
        df_processed['token_count'] = df_processed['tokens'].apply(len)
        
        # BoÅŸ metinleri filtrele
        initial_count = len(df_processed)
        df_processed = df_processed[df_processed['token_count'] > 0]
        final_count = len(df_processed)
        
        if initial_count != final_count:
            print(f"   ğŸ—‘ï¸ {initial_count - final_count} boÅŸ Ã¶rnek kaldÄ±rÄ±ldÄ±")
        
        # Label encoding (6-class -> binary iÃ§in)
        label_mapping = {
            'pants-fire': 0,    # Kesinlikle yanlÄ±ÅŸ
            'false': 0,         # YanlÄ±ÅŸ  
            'barely-true': 0,   # Neredeyse doÄŸru (yanlÄ±ÅŸ olarak kabul)
            'half-true': 1,     # YarÄ± doÄŸru
            'mostly-true': 1,   # Ã‡oÄŸunlukla doÄŸru
            'true': 1          # DoÄŸru
        }
        
        df_processed['binary_label'] = df_processed['label'].map(label_mapping)
        
        print(f"âœ… {dataset_name} Ã¶n iÅŸleme tamamlandÄ±!")
        print(f"   ğŸ“Š Final Ã¶rnek sayÄ±sÄ±: {len(df_processed):,}")
        print(f"   ğŸ“Š Label daÄŸÄ±lÄ±mÄ±:")
        print(f"      ğŸ”´ Fake (0): {(df_processed['binary_label'] == 0).sum():,}")
        print(f"      ğŸŸ¢ Real (1): {(df_processed['binary_label'] == 1).sum():,}")
        
        return df_processed
    
    def save_processed_data(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame):
        """
        Ä°ÅŸlenmiÅŸ veriyi kaydet
        
        Args:
            train_df: Ä°ÅŸlenmiÅŸ train verisi
            valid_df: Ä°ÅŸlenmiÅŸ validation verisi  
            test_df: Ä°ÅŸlenmiÅŸ test verisi
        """
        print("ğŸ’¾ Ä°ÅŸlenmiÅŸ veriler kaydediliyor...")
        
        processed_path = self.config['data']['processed_data_path']
        os.makedirs(processed_path, exist_ok=True)
        
        # CSV olarak kaydet
        train_df.to_csv(os.path.join(processed_path, "train_processed.csv"), index=False)
        valid_df.to_csv(os.path.join(processed_path, "valid_processed.csv"), index=False)
        test_df.to_csv(os.path.join(processed_path, "test_processed.csv"), index=False)
        
        # Pickle olarak da kaydet (daha hÄ±zlÄ± yÃ¼kleme iÃ§in)
        train_df.to_pickle(os.path.join(processed_path, "train_processed.pkl"))
        valid_df.to_pickle(os.path.join(processed_path, "valid_processed.pkl"))
        test_df.to_pickle(os.path.join(processed_path, "test_processed.pkl"))
        
        print("âœ… Ä°ÅŸlenmiÅŸ veriler kaydedildi!")
        print(f"   ğŸ“ Konum: {processed_path}")
    
    def get_data_statistics(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, test_df: pd.DataFrame) -> Dict:
        """
        Veri istatistikleri
        
        Returns:
            Dict: Ä°statistik bilgileri
        """
        stats = {
            'datasets': {
                'train': {
                    'count': len(train_df),
                    'fake_count': (train_df['binary_label'] == 0).sum(),
                    'real_count': (train_df['binary_label'] == 1).sum(),
                    'avg_token_count': train_df['token_count'].mean()
                },
                'valid': {
                    'count': len(valid_df),
                    'fake_count': (valid_df['binary_label'] == 0).sum(),
                    'real_count': (valid_df['binary_label'] == 1).sum(),
                    'avg_token_count': valid_df['token_count'].mean()
                },
                'test': {
                    'count': len(test_df),
                    'fake_count': (test_df['binary_label'] == 0).sum(),
                    'real_count': (test_df['binary_label'] == 1).sum(),
                    'avg_token_count': test_df['token_count'].mean()
                }
            }
        }
        
        return stats
    
    def run_full_preprocessing(self) -> bool:
        """
        Tam Ã¶n iÅŸleme pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r
        
        Returns:
            bool: Ä°ÅŸlem baÅŸarÄ±lÄ± mÄ±
        """
        print("ğŸš€ Tam veri Ã¶n iÅŸleme baÅŸlatÄ±lÄ±yor...\n")
        
        # 1. Veriyi indir
        if not self.download_liar_dataset():
            return False
        
        # 2. Veriyi yÃ¼kle
        train_df, valid_df, test_df = self.load_liar_data()
        if train_df is None:
            return False
        
        print()
        
        # 3. Tqdm'i etkinleÅŸtir
        tqdm.pandas()
        
        # 4. Her dataset'i Ã¶n iÅŸleme
        train_processed = self.preprocess_dataframe(train_df, "Train")
        print()
        valid_processed = self.preprocess_dataframe(valid_df, "Valid")
        print()
        test_processed = self.preprocess_dataframe(test_df, "Test")
        print()
        
        # 5. Ä°ÅŸlenmiÅŸ veriyi kaydet
        self.save_processed_data(train_processed, valid_processed, test_processed)
        print()
        
        # 6. Ä°statistikleri gÃ¶ster
        stats = self.get_data_statistics(train_processed, valid_processed, test_processed)
        print("ğŸ“Š Final Ä°statistikler:")
        for dataset_name, dataset_stats in stats['datasets'].items():
            print(f"   {dataset_name.upper()}:")
            print(f"      ğŸ“ˆ Toplam: {dataset_stats['count']:,}")
            print(f"      ğŸ”´ Fake: {dataset_stats['fake_count']:,} ({dataset_stats['fake_count']/dataset_stats['count']*100:.1f}%)")
            print(f"      ğŸŸ¢ Real: {dataset_stats['real_count']:,} ({dataset_stats['real_count']/dataset_stats['count']*100:.1f}%)")
            print(f"      ğŸ“ Avg Tokens: {dataset_stats['avg_token_count']:.1f}")
            print()
        
        print("ğŸ‰ Veri Ã¶n iÅŸleme tamamlandÄ±!")
        return True

# Test fonksiyonu
def main():
    """Test fonksiyonu"""
    preprocessor = DataPreprocessor()
    success = preprocessor.run_full_preprocessing()
    
    if success:
        print("âœ… Veri Ã¶n iÅŸleme baÅŸarÄ±yla tamamlandÄ±!")
    else:
        print("âŒ Veri Ã¶n iÅŸleme sÄ±rasÄ±nda hata oluÅŸtu!")

if __name__ == "__main__":
    main()