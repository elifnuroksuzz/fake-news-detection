"""
utils.py
YardÄ±mcÄ± fonksiyonlar ve genel kullanÄ±m araÃ§larÄ±
"""

import os
import json
import pickle
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

class Logger:
    """Basit logger sÄ±nÄ±fÄ±"""
    
    def __init__(self, log_file: Optional[str] = None):
        self.log_file = log_file
        
    def log(self, message: str, level: str = "INFO"):
        """Log mesajÄ± yaz"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {level}: {message}"
        
        print(log_message)
        
        if self.log_file:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_message + '\n')

class FileManager:
    """Dosya yÃ¶netimi iÃ§in yardÄ±mcÄ± sÄ±nÄ±f"""
    
    @staticmethod
    def create_directories(paths: List[str]):
        """Dizinleri oluÅŸtur"""
        for path in paths:
            os.makedirs(path, exist_ok=True)
            print(f"ğŸ“ Dizin oluÅŸturuldu: {path}")
    
    @staticmethod
    def save_json(data: Dict, filepath: str):
        """JSON dosyasÄ± kaydet"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ JSON kaydedildi: {filepath}")
    
    @staticmethod
    def load_json(filepath: str) -> Dict:
        """JSON dosyasÄ± yÃ¼kle"""
        with open(filepath, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    @staticmethod
    def save_pickle(obj: Any, filepath: str):
        """Pickle dosyasÄ± kaydet"""
        with open(filepath, 'wb') as f:
            pickle.dump(obj, f)
        print(f"ğŸ’¾ Pickle kaydedildi: {filepath}")
    
    @staticmethod
    def load_pickle(filepath: str) -> Any:
        """Pickle dosyasÄ± yÃ¼kle"""
        with open(filepath, 'rb') as f:
            return pickle.load(f)

class MetricsCalculator:
    """DeÄŸerlendirme metriklerini hesapla"""
    
    @staticmethod
    def calculate_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_pred_proba: Optional[np.ndarray] = None) -> Dict:
        """
        SÄ±nÄ±flandÄ±rma metrikleri hesapla
        
        Args:
            y_true: GerÃ§ek etiketler
            y_pred: Tahmin edilen etiketler
            y_pred_proba: Tahmin olasÄ±lÄ±klarÄ±
            
        Returns:
            Dict: Metrikler sÃ¶zlÃ¼ÄŸÃ¼
        """
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, 
            f1_score, roc_auc_score, confusion_matrix,
            classification_report
        )
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='binary'),
            'recall': recall_score(y_true, y_pred, average='binary'),
            'f1_score': f1_score(y_true, y_pred, average='binary'),
            'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
            'classification_report': classification_report(y_true, y_pred, output_dict=True)
        }
        
        if y_pred_proba is not None:
            metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)
        
        return metrics
    
    @staticmethod
    def print_metrics(metrics: Dict, model_name: str = "Model"):
        """Metrikleri yazdÄ±r"""
        print(f"\nğŸ“Š {model_name} Performans Metrikleri:")
        print(f"   ğŸ¯ Accuracy:  {metrics['accuracy']:.4f}")
        print(f"   ğŸ” Precision: {metrics['precision']:.4f}")
        print(f"   ğŸ“ˆ Recall:    {metrics['recall']:.4f}")
        print(f"   âš–ï¸ F1-Score:  {metrics['f1_score']:.4f}")
        if 'roc_auc' in metrics:
            print(f"   ğŸ“Š ROC-AUC:   {metrics['roc_auc']:.4f}")

class Visualizer:
    """GÃ¶rselleÅŸtirme fonksiyonlarÄ±"""
    
    def __init__(self, figsize: Tuple[int, int] = (12, 8), style: str = 'whitegrid'):
        """
        GÃ¶rselleÅŸtirici baÅŸlat
        
        Args:
            figsize: Figure boyutu
            style: Seaborn stili
        """
        plt.style.use('default')
        sns.set_style(style)
        self.figsize = figsize
    
    def plot_confusion_matrix(self, cm: np.ndarray, labels: List[str], title: str = "Confusion Matrix", save_path: Optional[str] = None):
        """
        Confusion matrix Ã§iz
        
        Args:
            cm: Confusion matrix
            labels: SÄ±nÄ±f etiketleri
            title: Grafik baÅŸlÄ±ÄŸÄ±
            save_path: Kaydetme yolu
        """
        fig, ax = plt.subplots(figsize=self.figsize)
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=labels, yticklabels=labels, ax=ax)
        
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.set_xlabel('Predicted', fontsize=12)
        ax.set_ylabel('Actual', fontsize=12)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Confusion matrix kaydedildi: {save_path}")
        
        plt.show()
    
    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray, auc_score: float, 
                      title: str = "ROC Curve", save_path: Optional[str] = None):
        """
        ROC Curve Ã§iz
        
        Args:
            fpr: False positive rate
            tpr: True positive rate
            auc_score: AUC skoru
            title: Grafik baÅŸlÄ±ÄŸÄ±
            save_path: Kaydetme yolu
        """
        fig, ax = plt.subplots(figsize=self.figsize)
        
        ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (AUC = {auc_score:.4f})')
        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
        
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontsize=12)
        ax.set_ylabel('True Positive Rate', fontsize=12)
        ax.set_title(title, fontsize=16, fontweight='bold')
        ax.legend(loc="lower right")
        ax.grid(alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ROC Curve kaydedildi: {save_path}")
        
        plt.show()
    
    def plot_training_history(self, history: Dict, metrics: List[str], 
                            title: str = "Training History", save_path: Optional[str] = None):
        """
        EÄŸitim geÃ§miÅŸi grafiÄŸi Ã§iz
        
        Args:
            history: EÄŸitim geÃ§miÅŸi
            metrics: GÃ¶sterilecek metrikler
            title: Grafik baÅŸlÄ±ÄŸÄ±
            save_path: Kaydetme yolu
        """
        fig, axes = plt.subplots(1, len(metrics), figsize=(6*len(metrics), 6))
        
        if len(metrics) == 1:
            axes = [axes]
        
        for i, metric in enumerate(metrics):
            if metric in history:
                axes[i].plot(history[metric], label=f'Train {metric}')
                if f'val_{metric}' in history:
                    axes[i].plot(history[f'val_{metric}'], label=f'Validation {metric}')
                
                axes[i].set_title(f'{metric.capitalize()}', fontsize=14)
                axes[i].set_xlabel('Epoch', fontsize=12)
                axes[i].set_ylabel(metric.capitalize(), fontsize=12)
                axes[i].legend()
                axes[i].grid(alpha=0.3)
        
        plt.suptitle(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Training history kaydedildi: {save_path}")
        
        plt.show()
    
    def plot_feature_importance(self, feature_names: List[str], importance_scores: np.ndarray, 
                               top_n: int = 20, title: str = "Feature Importance", save_path: Optional[str] = None):
        """
        Ã–zellik Ã¶nemliliÄŸi grafiÄŸi
        
        Args:
            feature_names: Ã–zellik isimleri
            importance_scores: Ã–nemlilik skorlarÄ±
            top_n: GÃ¶sterilecek Ã¶zellik sayÄ±sÄ±
            title: Grafik baÅŸlÄ±ÄŸÄ±
            save_path: Kaydetme yolu
        """
        # En Ã¶nemli Ã¶zellikleri seÃ§
        indices = np.argsort(importance_scores)[::-1][:top_n]
        
        fig, ax = plt.subplots(figsize=self.figsize)
        
        ax.barh(range(len(indices)), importance_scores[indices])
        ax.set_yticks(range(len(indices)))
        ax.set_yticklabels([feature_names[i] for i in indices])
        ax.set_xlabel('Importance Score', fontsize=12)
        ax.set_title(f'{title} (Top {top_n})', fontsize=16, fontweight='bold')
        
        # En Ã¶nemli Ã¶zellikleri vurgula
        for i, (idx, score) in enumerate(zip(indices, importance_scores[indices])):
            ax.text(score + score*0.01, i, f'{score:.4f}', va='center')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š Feature importance kaydedildi: {save_path}")
        
        plt.show()

class ConfigManager:
    """KonfigÃ¼rasyon yÃ¶netimi"""
    
    @staticmethod
    def load_config(config_path: str = "config.yaml") -> Dict:
        """KonfigÃ¼rasyon dosyasÄ±nÄ± yÃ¼kle"""
        try:
            with open(config_path, 'r', encoding='utf-8') as file:
                return yaml.safe_load(file)
        except FileNotFoundError:
            print(f"âš ï¸ KonfigÃ¼rasyon dosyasÄ± bulunamadÄ±: {config_path}")
            return {}
    
    @staticmethod
    def save_config(config: Dict, config_path: str = "config.yaml"):
        """KonfigÃ¼rasyonu kaydet"""
        with open(config_path, 'w', encoding='utf-8') as file:
            yaml.dump(config, file, default_flow_style=False, allow_unicode=True)
        print(f"ğŸ’¾ KonfigÃ¼rasyon kaydedildi: {config_path}")

class DataSplitter:
    """Veri bÃ¶lme iÅŸlemleri"""
    
    @staticmethod
    def stratified_train_test_split(X: pd.DataFrame, y: pd.Series, 
                                  test_size: float = 0.2, random_state: int = 42) -> Tuple:
        """
        Stratified train-test split
        
        Args:
            X: Ã–zellikler
            y: Etiketler
            test_size: Test boyutu
            random_state: Random seed
            
        Returns:
            Tuple: X_train, X_test, y_train, y_test
        """
        from sklearn.model_selection import train_test_split
        
        return train_test_split(X, y, test_size=test_size, 
                              stratify=y, random_state=random_state)
    
    @staticmethod
    def create_k_fold_splits(X: pd.DataFrame, y: pd.Series, 
                           n_splits: int = 10, random_state: int = 42):
        """
        K-Fold cross validation splits oluÅŸtur
        
        Args:
            X: Ã–zellikler
            y: Etiketler
            n_splits: Fold sayÄ±sÄ±
            random_state: Random seed
            
        Returns:
            StratifiedKFold: Cross validation objesi
        """
        from sklearn.model_selection import StratifiedKFold
        
        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

class ModelPersistence:
    """Model kaydetme ve yÃ¼kleme"""
    
    @staticmethod
    def save_model(model, model_path: str, metadata: Optional[Dict] = None):
        """
        Model kaydet
        
        Args:
            model: Kaydedilecek model
            model_path: KayÄ±t yolu
            metadata: Model metadata'sÄ±
        """
        import joblib
        
        # Model kaydet
        joblib.dump(model, model_path)
        print(f"ğŸ’¾ Model kaydedildi: {model_path}")
        
        # Metadata kaydet
        if metadata:
            metadata_path = model_path.replace('.pkl', '_metadata.json')
            FileManager.save_json(metadata, metadata_path)
    
    @staticmethod
    def load_model(model_path: str):
        """
        Model yÃ¼kle
        
        Args:
            model_path: Model yolu
            
        Returns:
            Loaded model
        """
        import joblib
        
        model = joblib.load(model_path)
        print(f"ğŸ“¥ Model yÃ¼klendi: {model_path}")
        
        return model
    
    @staticmethod
    def load_model_with_metadata(model_path: str) -> Tuple:
        """
        Model ve metadata'yÄ± yÃ¼kle
        
        Args:
            model_path: Model yolu
            
        Returns:
            Tuple: (model, metadata)
        """
        import joblib
        
        # Model yÃ¼kle
        model = joblib.load(model_path)
        
        # Metadata yÃ¼kle
        metadata_path = model_path.replace('.pkl', '_metadata.json')
        metadata = None
        
        if os.path.exists(metadata_path):
            metadata = FileManager.load_json(metadata_path)
        
        print(f"ğŸ“¥ Model ve metadata yÃ¼klendi: {model_path}")
        
        return model, metadata

class PerformanceTracker:
    """Performans takibi"""
    
    def __init__(self):
        self.results = {}
    
    def add_result(self, model_name: str, fold: int, metrics: Dict):
        """
        SonuÃ§ ekle
        
        Args:
            model_name: Model adÄ±
            fold: Fold numarasÄ±
            metrics: Metrikler
        """
        if model_name not in self.results:
            self.results[model_name] = []
        
        result = {'fold': fold, **metrics}
        self.results[model_name].append(result)
    
    def get_average_metrics(self, model_name: str) -> Dict:
        """
        Ortalama metrikler
        
        Args:
            model_name: Model adÄ±
            
        Returns:
            Dict: Ortalama metrikler
        """
        if model_name not in self.results:
            return {}
        
        results = self.results[model_name]
        
        # Metrik isimlerini al (fold dÄ±ÅŸÄ±nda)
        metric_names = [key for key in results[0].keys() if key != 'fold']
        
        avg_metrics = {}
        for metric in metric_names:
            if metric not in ['confusion_matrix', 'classification_report']:
                values = [result[metric] for result in results]
                avg_metrics[f'{metric}_mean'] = np.mean(values)
                avg_metrics[f'{metric}_std'] = np.std(values)
        
        return avg_metrics
    
    def get_best_model(self, metric: str = 'f1_score') -> str:
        """
        En iyi modeli bul
        
        Args:
            metric: KarÅŸÄ±laÅŸtÄ±rma metriÄŸi
            
        Returns:
            str: En iyi model adÄ±
        """
        best_model = None
        best_score = -1
        
        for model_name in self.results.keys():
            avg_metrics = self.get_average_metrics(model_name)
            metric_key = f'{metric}_mean'
            
            if metric_key in avg_metrics:
                score = avg_metrics[metric_key]
                if score > best_score:
                    best_score = score
                    best_model = model_name
        
        return best_model
    
    def print_summary(self):
        """Ã–zet rapor yazdÄ±r"""
        print("\nğŸ“Š MODEL PERFORMANS Ã–ZETÄ°")
        print("=" * 50)
        
        for model_name in self.results.keys():
            print(f"\nğŸ¤– {model_name}")
            avg_metrics = self.get_average_metrics(model_name)
            
            for metric, value in avg_metrics.items():
                if '_mean' in metric:
                    base_metric = metric.replace('_mean', '')
                    std_metric = f'{base_metric}_std'
                    mean_val = avg_metrics[metric]
                    std_val = avg_metrics.get(std_metric, 0)
                    print(f"   {base_metric.upper()}: {mean_val:.4f} Â± {std_val:.4f}")
        
        best_model = self.get_best_model('f1_score')
        if best_model:
            print(f"\nğŸ† EN Ä°YÄ° MODEL: {best_model}")
    
    def save_results(self, filepath: str):
        """SonuÃ§larÄ± kaydet"""
        FileManager.save_json(self.results, filepath)
    
    def load_results(self, filepath: str):
        """SonuÃ§larÄ± yÃ¼kle"""
        self.results = FileManager.load_json(filepath)

def setup_reproducibility(seed: int = 42):
    """
    Reproducibility iÃ§in random seed ayarla
    
    Args:
        seed: Random seed deÄŸeri
    """
    import random
    
    # Python random
    random.seed(seed)
    
    # NumPy random
    np.random.seed(seed)
    
    # Scikit-learn iÃ§indeki random durumlar iÃ§in
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"ğŸ”’ Reproducibility ayarlandÄ± (seed: {seed})")

def print_system_info():
    """Sistem bilgilerini yazdÄ±r"""
    import sys
    import platform
    
    print("ğŸ’» SÄ°STEM BÄ°LGÄ°LERÄ°")
    print("=" * 30)
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ’» Platform: {platform.platform()}")
    print(f"ğŸ–¥ï¸ Processor: {platform.processor()}")
    print(f"ğŸ“ Current Dir: {os.getcwd()}")
    
    # Paket versiyonlarÄ±
    packages = ['numpy', 'pandas', 'scikit-learn', 'matplotlib', 'seaborn']
    print("\nğŸ“¦ PAKET VERSÄ°YONLARI")
    print("=" * 30)
    
    for package in packages:
        try:
            module = __import__(package)
            version = getattr(module, '__version__', 'Unknown')
            print(f"ğŸ“š {package}: {version}")
        except ImportError:
            print(f"âŒ {package}: Not installed")

def create_project_structure():
    """Proje klasÃ¶r yapÄ±sÄ±nÄ± oluÅŸtur"""
    directories = [
        'data/raw',
        'data/processed', 
        'data/external',
        'models/trained_models',
        'models/model_configs',
        'results/figures',
        'results/reports',
        'results/metrics',
        'notebooks'
    ]
    
    FileManager.create_directories(directories)
    print("ğŸ‰ Proje yapÄ±sÄ± oluÅŸturuldu!")

# Test fonksiyonu
def test_utils():
    """Utils modÃ¼lÃ¼nÃ¼ test et"""
    print("ğŸ§ª Utils modÃ¼lÃ¼ test ediliyor...")
    
    # System info
    print_system_info()
    print()
    
    # Reproducibility
    setup_reproducibility(42)
    print()
    
    # Performance tracker test
    tracker = PerformanceTracker()
    tracker.add_result('TestModel', 1, {'accuracy': 0.85, 'f1_score': 0.83})
    tracker.add_result('TestModel', 2, {'accuracy': 0.87, 'f1_score': 0.85})
    
    avg_metrics = tracker.get_average_metrics('TestModel')
    print("ğŸ“Š Test metrics:", avg_metrics)
    
    print("âœ… Utils modÃ¼lÃ¼ test tamamlandÄ±!")

if __name__ == "__main__":
    test_utils()