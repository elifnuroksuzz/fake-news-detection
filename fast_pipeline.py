"""
fast_pipeline.py
Optimize edilmiÅŸ, hÄ±zlÄ± Ã§alÄ±ÅŸan fake news detection pipeline
DonanÄ±m-optimized, minimal hyperparameter tuning
"""

import sys
import os
import time
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple
import pickle
import warnings
warnings.filterwarnings('ignore')

# DonanÄ±m optimizasyonu
os.environ['OMP_NUM_THREADS'] = '4'  # CPU core limit
os.environ['NUMBA_NUM_THREADS'] = '4'

# src modÃ¼llerini import et
sys.path.append('src')

class FastFakeNewsDetector:
    """
    HÄ±zlÄ± fake news detection sistemi
    Minimal hyperparameter tuning, maksimum performans
    """
    
    def __init__(self):
        """Pipeline baÅŸlatÄ±cÄ±"""
        self.models = {}
        self.results = {}
        self.feature_names = []
        
    def load_prepared_features(self) -> bool:
        """HazÄ±rlanmÄ±ÅŸ feature'larÄ± yÃ¼kle"""
        try:
            print("ğŸ“¥ HazÄ±r feature'lar yÃ¼kleniyor...")
            
            with open('data/processed/features_ready.pkl', 'rb') as f:
                data = pickle.load(f)
            
            self.features = data['features']
            self.feature_names = data['feature_names']
            
            print(f"âœ… Feature'lar yÃ¼klendi:")
            print(f"   ğŸ“Š Train: {self.features['X_train'].shape}")
            print(f"   ğŸ“Š Valid: {self.features['X_valid'].shape}")
            print(f"   ğŸ“Š Test: {self.features['X_test'].shape}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Feature yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def train_optimized_models(self) -> Dict:
        """
        Optimize edilmiÅŸ modelleri hÄ±zlÄ±ca eÄŸit
        Minimal hyperparameter tuning, iyi performans
        """
        print("\nğŸš€ HIZLI MODEL EÄÄ°TÄ°MÄ° BAÅLATILIYOR!")
        print("=" * 60)
        
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.linear_model import LogisticRegression
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, 
            f1_score, roc_auc_score, classification_report
        )
        
        X_train = self.features['X_train']
        y_train = self.features['y_train']
        X_test = self.features['X_test'] 
        y_test = self.features['y_test']
        
        # Model tanÄ±mlarÄ± - optimize edilmiÅŸ parametreler
        model_configs = {
            'RandomForest': {
                'model': RandomForestClassifier(
                    n_estimators=200,
                    max_depth=20,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    max_features='sqrt',
                    random_state=42,
                    n_jobs=-1,  # TÃ¼m core'larÄ± kullan
                    class_weight='balanced'
                ),
                'name': 'Random Forest'
            },
            
            'LogisticRegression': {
                'model': LogisticRegression(
                    C=1.0,
                    penalty='l2',
                    solver='saga',
                    max_iter=2000,
                    random_state=42,
                    n_jobs=-1,
                    class_weight='balanced'
                ),
                'name': 'Logistic Regression'
            }
        }
        
        # XGBoost (varsa)
        try:
            import xgboost as xgb
            model_configs['XGBoost'] = {
                'model': xgb.XGBClassifier(
                    n_estimators=200,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.9,
                    colsample_bytree=0.9,
                    random_state=42,
                    n_jobs=-1,
                    verbosity=0
                ),
                'name': 'XGBoost'
            }
        except ImportError:
            pass
        
        # LightGBM (varsa)
        try:
            import lightgbm as lgb
            model_configs['LightGBM'] = {
                'model': lgb.LGBMClassifier(
                    n_estimators=200,
                    max_depth=6,
                    learning_rate=0.1,
                    num_leaves=63,
                    subsample=0.9,
                    random_state=42,
                    n_jobs=-1,
                    verbosity=-1
                ),
                'name': 'LightGBM'
            }
        except ImportError:
            pass
        
        results = {}
        
        # Her modeli eÄŸit ve test et
        for model_key, config in model_configs.items():
            print(f"\nğŸ¤– {config['name']} eÄŸitiliyor...")
            start_time = time.time()
            
            model = config['model']
            
            # EÄŸitim
            model.fit(X_train, y_train)
            
            # Test tahminleri
            y_pred = model.predict(X_test)
            y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
            
            # Metrikler
            metrics = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),
                'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),
                'f1_score': f1_score(y_test, y_pred, average='binary', zero_division=0)
            }
            
            if y_pred_proba is not None:
                metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba)
            
            # EÄŸitim sÃ¼resi
            training_time = time.time() - start_time
            metrics['training_time'] = training_time
            
            # SonuÃ§larÄ± kaydet
            results[model_key] = {
                'model': model,
                'metrics': metrics,
                'predictions': y_pred,
                'probabilities': y_pred_proba
            }
            
            self.models[model_key] = model
            
            # SonuÃ§larÄ± yazdÄ±r
            print(f"   â±ï¸ EÄŸitim sÃ¼resi: {training_time:.2f} saniye")
            print(f"   ğŸ¯ Accuracy:  {metrics['accuracy']:.4f}")
            print(f"   ğŸ” Precision: {metrics['precision']:.4f}")
            print(f"   ğŸ“ˆ Recall:    {metrics['recall']:.4f}")
            print(f"   âš–ï¸ F1-Score:  {metrics['f1_score']:.4f}")
            if 'roc_auc' in metrics:
                print(f"   ğŸ“Š ROC-AUC:   {metrics['roc_auc']:.4f}")
        
        # En iyi modeli belirle
        best_f1 = 0
        best_model_key = ""
        
        for model_key, result in results.items():
            f1 = result['metrics']['f1_score']
            if f1 > best_f1:
                best_f1 = f1
                best_model_key = model_key
        
        print(f"\nğŸ† EN Ä°YÄ° MODEL: {model_configs[best_model_key]['name']}")
        print(f"ğŸ¯ En Ä°yi F1-Score: {best_f1:.4f}")
        
        self.results = results
        self.best_model_key = best_model_key
        self.best_model = results[best_model_key]['model']
        
        return results
    
    def create_quick_visualizations(self):
        """HÄ±zlÄ± gÃ¶rselleÅŸtirmeler oluÅŸtur"""
        print("\nğŸ“Š HÄ±zlÄ± gÃ¶rselleÅŸtirmeler oluÅŸturuluyor...")
        
        import matplotlib.pyplot as plt
        import seaborn as sns
        from sklearn.metrics import confusion_matrix, roc_curve
        
        # Stil ayarlarÄ±
        plt.style.use('default')
        sns.set_palette("husl")
        
        os.makedirs('results/figures', exist_ok=True)
        
        y_test = self.features['y_test']
        
        # Model karÅŸÄ±laÅŸtÄ±rma grafiÄŸi
        fig, ax = plt.subplots(figsize=(12, 6))
        
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        model_names = []
        metric_values = {metric: [] for metric in metrics}
        
        for model_key, result in self.results.items():
            model_names.append(model_key)
            for metric in metrics:
                metric_values[metric].append(result['metrics'][metric])
        
        x = np.arange(len(model_names))
        width = 0.2
        
        for i, metric in enumerate(metrics):
            ax.bar(x + i * width, metric_values[metric], width, 
                  label=metric.upper().replace('_', '-'), alpha=0.8)
        
        ax.set_xlabel('Models')
        ax.set_ylabel('Score')
        ax.set_title('Model Performance Comparison')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(model_names)
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('results/figures/fast_model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # En iyi model iÃ§in confusion matrix
        best_result = self.results[self.best_model_key]
        cm = confusion_matrix(y_test, best_result['predictions'])
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['Fake', 'Real'],
                   yticklabels=['Fake', 'Real'], ax=ax)
        
        ax.set_title(f'{self.best_model_key} - Confusion Matrix')
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        
        plt.tight_layout()
        plt.savefig(f'results/figures/fast_{self.best_model_key.lower()}_confusion_matrix.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… GÃ¶rselleÅŸtirmeler kaydedildi:")
        print("   ğŸ“Š results/figures/fast_model_comparison.png")
        print("   ğŸ“Š results/figures/fast_{}_confusion_matrix.png".format(self.best_model_key.lower()))
    
    def save_models(self):
        """Modelleri kaydet"""
        print("\nğŸ’¾ Modeller kaydediliyor...")
        
        os.makedirs('models/trained_models', exist_ok=True)
        
        for model_key, model in self.models.items():
            filename = f'models/trained_models/fast_{model_key.lower()}_model.pkl'
            
            with open(filename, 'wb') as f:
                pickle.dump(model, f)
            
            print(f"   ğŸ’¾ {model_key} kaydedildi: {filename}")
        
        # En iyi model
        best_filename = f'models/trained_models/best_fast_model_{self.best_model_key.lower()}.pkl'
        with open(best_filename, 'wb') as f:
            pickle.dump(self.best_model, f)
        
        print(f"   ğŸ† En iyi model kaydedildi: {best_filename}")
    
    def demo_predictions(self):
        """Demo tahminler"""
        print("\nğŸ§ª DEMO TAHMÄ°NLER")
        print("=" * 40)
        
        demo_texts = [
            "The Federal Reserve announced interest rate changes to combat inflation.",
            "Scientists discover breakthrough in renewable energy storage technology.",
            "BREAKING: Aliens have officially contacted world governments!",
            "SHOCKING: Local man discovers secret to eternal youth!",
            "The unemployment rate decreased according to latest statistics."
        ]
        
        expected_labels = ['Real', 'Real', 'Fake', 'Fake', 'Real']
        
        from data_preprocessing import DataPreprocessor
        from feature_engineering import FeatureEngineering
        import yaml
        
        # Preprocessor ve feature engineer yÃ¼kle
        config = yaml.safe_load(open('config.yaml'))
        preprocessor = DataPreprocessor()
        feature_engineer = FeatureEngineering(config)
        
        # TF-IDF vectorizer'Ä± yÃ¼kle
        feature_engineer.tfidf_vectorizer = self.get_tfidf_vectorizer()
        
        correct = 0
        
        for i, (text, expected) in enumerate(zip(demo_texts, expected_labels), 1):
            try:
                # Metni iÅŸle
                cleaned = preprocessor.clean_text(text)
                tokens = preprocessor.tokenize_and_filter(cleaned)
                
                # Basit feature extraction (sadece TF-IDF)
                text_for_tfidf = ' '.join(tokens)
                features = feature_engineer.tfidf_vectorizer.transform([text_for_tfidf])
                
                # Sadece TF-IDF ile tahmin (diÄŸer feature'lar olmadan)
                if features.shape[1] < self.features['X_train'].shape[1]:
                    # Eksik sÃ¼tunlarÄ± sÄ±fÄ±r ile doldur
                    missing_cols = self.features['X_train'].shape[1] - features.shape[1]
                    padding = np.zeros((1, missing_cols))
                    features = np.hstack([features.toarray(), padding])
                
                prediction = self.best_model.predict(features)[0]
                confidence = max(self.best_model.predict_proba(features)[0])
                
                result = 'Real' if prediction == 1 else 'Fake'
                status = 'âœ…' if result == expected else 'âŒ'
                
                if result == expected:
                    correct += 1
                
                print(f"{i}. {text[:50]}...")
                print(f"   Tahmin: {result} ({confidence:.3f}) - GerÃ§ek: {expected} {status}")
                
            except Exception as e:
                print(f"{i}. Tahmin hatasÄ±: {e}")
        
        accuracy = correct / len(demo_texts)
        print(f"\nğŸ“Š Demo Accuracy: {accuracy:.1%} ({correct}/{len(demo_texts)})")
    
    def get_tfidf_vectorizer(self):
        """TF-IDF vectorizer'Ä± feature engineering'den al"""
        # Bu basit bir placeholder - gerÃ§ekte feature_engineer'den alÄ±nmalÄ±
        from sklearn.feature_extraction.text import TfidfVectorizer
        
        # EÄŸitim data'sÄ±ndan TF-IDF vectorizer'Ä± yeniden oluÅŸtur
        # (Normalde kaydedilmiÅŸ olmasÄ± gerek)
        
        return TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.95,
            stop_words='english'
        )
    
    def run_fast_pipeline(self) -> bool:
        """HÄ±zlÄ± pipeline Ã§alÄ±ÅŸtÄ±r"""
        start_time = time.time()
        
        print("ğŸš€ HIZLI FAKE NEWS DETECTION PIPELINE")
        print("=" * 60)
        print(f"â° BaÅŸlangÄ±Ã§: {datetime.now().strftime('%H:%M:%S')}")
        
        try:
            # 1. Feature'larÄ± yÃ¼kle
            if not self.load_prepared_features():
                return False
            
            # 2. Modelleri hÄ±zlÄ±ca eÄŸit
            self.train_optimized_models()
            
            # 3. GÃ¶rselleÅŸtirmeler
            self.create_quick_visualizations()
            
            # 4. Modelleri kaydet
            self.save_models()
            
            # 5. Demo tahminler
            self.demo_predictions()
            
            # Toplam sÃ¼re
            total_time = time.time() - start_time
            
            print("\n" + "=" * 60)
            print("ğŸ‰ HIZLI PIPELINE TAMAMLANDI!")
            print(f"â±ï¸ Toplam sÃ¼re: {total_time/60:.1f} dakika")
            print(f"ğŸ† En iyi model: {self.best_model_key}")
            print(f"ğŸ¯ En iyi F1-Score: {self.results[self.best_model_key]['metrics']['f1_score']:.4f}")
            print("=" * 60)
            
            return True
            
        except Exception as e:
            print(f"âŒ Pipeline hatasÄ±: {e}")
            return False

def main():
    """Ana fonksiyon"""
    detector = FastFakeNewsDetector()
    success = detector.run_fast_pipeline()
    
    if success:
        print("\nâœ… Sistem kullanÄ±ma hazÄ±r!")
    else:
        print("\nâŒ Pipeline baÅŸarÄ±sÄ±z!")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())